{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading libraries\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "#data preprocessing\n",
    "\n",
    "datatrain=pd.read_csv('Iris.csv')\n",
    "datatrain.loc[datatrain['Species']=='Iris-setosa', 'Species']=0\n",
    "datatrain.loc[datatrain['Species']=='Iris-versicolor', 'Species']=1\n",
    "datatrain.loc[datatrain['Species']=='Iris-virginica', 'Species']=2\n",
    "datatrain = datatrain.apply(pd.to_numeric)\n",
    "datatrain=np.array(datatrain)\n",
    "\n",
    "\n",
    "\n",
    "#split x and y (feature and target)\n",
    "X = datatrain[:,1:5]\n",
    "y = datatrain[:,5]\n",
    "\n",
    "y=pd.get_dummies(y)\n",
    "y=np.array(y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y)\n",
    "\n",
    "X_train=torch.from_numpy(X_train)\n",
    "y_train=torch.from_numpy(y_train)\n",
    "X_test=torch.from_numpy(X_test)\n",
    "y_test=torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom loss function that penalizes large weights\n",
    "class MyLoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,eps1,eps2,beta):\n",
    "        super(MyLoss,self).__init__()\n",
    "        self.eps1=eps1\n",
    "        self.eps2=eps2\n",
    "        self.beta=beta\n",
    "        \n",
    "    def forward(self,target,cat_label):\n",
    "        ploss=self.eps1*(torch.sum((self.beta*(net.fc1.weight.clone()**2))/(1+(self.beta*(net.fc1.weight.clone()**2))))+torch.sum((self.beta*(net.fc2.weight.clone()**2))/(1+(self.beta*(net.fc2.weight.clone()**2)))))\n",
    "        +self.eps2*torch.sum(net.fc1.weight.clone()**2)+self.eps2*torch.sum(net.fc2.weight.clone()**2)\n",
    "        celoss=-(torch.sum(cat_label * torch.log(target) + (1 - cat_label) * torch.log(1-target)))\n",
    "        totloss=celoss+ploss\n",
    "        \n",
    "        return totloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to drop connections between two neurons\n",
    "class MaskedLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(MaskedLinear, self).__init__(in_features, out_features, bias)\n",
    "        self.mask_flag = False\n",
    "    \n",
    "    def set_mask(self, mask):\n",
    "        self.register_buffer('mask', mask)\n",
    "        mask_var = self.get_mask()\n",
    "        self.weight.data = self.weight.data*mask_var.data\n",
    "        self.mask_flag = True\n",
    "    \n",
    "    def get_mask(self):\n",
    "        # print(self.mask_flag)\n",
    "        return Variable(self.mask, requires_grad=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.mask_flag == True:\n",
    "            mask_var = self.get_mask()\n",
    "            weight = self.weight * mask_var\n",
    "            return F.linear(x, weight, self.bias)\n",
    "        else:\n",
    "            return F.linear(x, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network definition\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = MaskedLinear(input_size, hidden_size,bias=1) \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = MaskedLinear(hidden_size, num_classes,bias=1)\n",
    "        self.sigm=nn.Sigmoid()\n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.fc1(x)\n",
    "        x=self.tanh(x)\n",
    "        out=self.fc2(x)\n",
    "        out=self.sigm(out)\n",
    "        \n",
    "        return out\n",
    "    def store_neuron_outputs(self,x):\n",
    "        x=self.fc1(x)\n",
    "        x=self.tanh(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def set_masks(self, masks):\n",
    "        self.fc1.set_mask(masks[0])\n",
    "        self.fc2.set_mask(masks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "net =Net(4,1,3)\n",
    "criterion = MyLoss(0.01,0.01,10)\n",
    "#criterion=nn.NLLLoss()\n",
    "optimizer = torch.optim.RMSprop(net.parameters(), lr=0.001)\n",
    "X_tr = Variable(X_train).float()\n",
    "Y_hot = Variable(y_train).float()\n",
    "Y_cat=np.where(y_train==1)[1]\n",
    "Y_cat = Variable(torch.from_numpy(Y_cat)).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that trains a network and prunes connections with least importance until it does not improve accuracy\n",
    "def prune_network(net,no_of_neurons):\n",
    "    maskfc1=torch.ones_like(model.fc1.weight.data)\n",
    "    maskfc2=torch.ones_like(model.fc2.weight.data)\n",
    "    masks=[]\n",
    "    masks.append(maskfc1)\n",
    "    masks.append(maskfc2)\n",
    "    while(1):\n",
    "        removed=False\n",
    "        for m in range(no_neurons):\n",
    "            for l in range(net.fc1.weight.data.shape[1]):\n",
    "                wml=net.fc1.weight.clone()[m,:][l]\n",
    "                if(torch.max(torch.abs(net.fc1.weight.clone()[m,:][0]*net.fc2.weight.clone()[:,m]))<4*neta2):\n",
    "                    masks=remove(net,masks,\"fc1\",m,l)\n",
    "                    removed=True\n",
    "            for p in range(net.fc2.weight.data.shape[0]):\n",
    "                vpm=net.fc2.weight.clone()[:,m][p]\n",
    "                if(torch.abs(vpm)<4*neta2):\n",
    "                    masks=remove(net,masks,\"fc2\",m,p)\n",
    "                    removed=True\n",
    "        if(removed==True):\n",
    "            net.set_masks(masks)\n",
    "            train(net)\n",
    "            if(accuracy<prev_accuracy):\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            WML=[]\n",
    "            for m in range(no_neurons):\n",
    "                for l in range(net.fc1.weight.data.shape[1]):\n",
    "                    wml=net.fc1.weight.clone()[m,:][l]\n",
    "                    wml=torch.max(torch.abs(net.fc1.weight.clone()[m,:][0]*net.fc2.weight.clone()[:,m]))\n",
    "                    WML.append(wml)\n",
    "            remove(min(WML))\n",
    "        net.set_masks(masks)\n",
    "        train(net)\n",
    "        if(accuracy<prev_accuracy):\n",
    "            continue\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove specified connections\n",
    "def remove(model,prev_masks,layer,m,l):\n",
    "    curr_masks=[]\n",
    "    mask1=torch.ones_like(model.fc1.weight.data)\n",
    "    mask2=torch.ones_like(model.fc2.weight.data)\n",
    "    if(layer==\"fc1\"):\n",
    "        mask1[m,:][l]=0\n",
    "    elif(layer==\"fc2\"):\n",
    "        mask2[:,m][l]=0\n",
    "    curr_masks.append(prev_masks[0]*mask1)\n",
    "    curr_masks.append(prev_masks[1]*mask2)\n",
    "    \n",
    "    return curr_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trains the model \n",
    "def train(model,no_epochs,loss_fn,optmizer):\n",
    "    for i in range(no_of_epochs):\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        out =model(X)\n",
    "        loss = loss_fn(out,Y_hot)\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        print(loss.data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
